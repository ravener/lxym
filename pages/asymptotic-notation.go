
package pages

const Asymptotic_Notation = "# Asymptotic Notations\n\n## What are they?\n\nAsymptotic Notations are languages that allow us to analyze an algorithm's \nrunning time by identifying its behavior as the input size for the algorithm \nincreases. This is also known as an algorithm's growth rate. Does the \nalgorithm suddenly become incredibly slow when the input size grows? Does it \nmostly maintain its quick run time as the input size increases? Asymptotic \nNotation gives us the ability to answer these questions.\n\n## Are there alternatives to answering these questions?\n\nOne way would be to count the number of primitive operations at different \ninput sizes. Though this is a valid solution, the amount of work this takes \nfor even simple algorithms does not justify its use.\n\nAnother way is to physically measure the amount of time an algorithm takes to \ncomplete given different input sizes. However, the accuracy and relativity \n(times obtained would only be relative to the machine they were computed on) \nof this method is bound to environmental variables such as computer hardware \nspecifications, processing power, etc.\n\n## Types of Asymptotic Notation\n\nIn the first section of this doc, we described how an Asymptotic Notation \nidentifies the behavior of an algorithm as the input size changes. Let us \nimagine an algorithm as a function f, n as the input size, and f(n) being \nthe running time. So for a given algorithm f, with input size n you get \nsome resultant run time f(n). This results in a graph where the Y-axis is \nthe runtime, the X-axis is the input size, and plot points are the resultants \nof the amount of time for a given input size.\n\nYou can label a function, or algorithm, with an Asymptotic Notation in many \ndifferent ways. Some examples are, you can describe an algorithm by its best \ncase, worst case, or average case. The most common is to analyze an algorithm \nby its worst case. You typically don’t evaluate by best case because those \nconditions aren’t what you’re planning for. An excellent example of this is \nsorting algorithms; particularly, adding elements to a tree structure. The \nbest case for most algorithms could be as low as a single operation. However, \nin most cases, the element you’re adding needs to be sorted appropriately \nthrough the tree, which could mean examining an entire branch. This is \nthe worst case, and this is what we plan for.\n\n### Types of functions, limits, and simplification\n\n```\nLogarithmic Function - log n\nLinear Function - an + b\nQuadratic Function - an^2 + bn + c\nPolynomial Function - an^z + . . . + an^2 + a*n^1 + a*n^0, where z is some \nconstant\nExponential Function - a^n, where a is some constant\n```\n\nThese are some fundamental function growth classifications used in \nvarious notations. The list starts at the slowest growing function \n(logarithmic, fastest execution time) and goes on to the fastest \ngrowing (exponential, slowest execution time). Notice that as ‘n’ \nor the input, increases in each of those functions, the result \nincreases much quicker in quadratic, polynomial, and exponential, \ncompared to logarithmic and linear.\n\nIt is worth noting that for the notations about to be discussed, \nyou should do your best to use the simplest terms. This means to \ndisregard constants, and lower order terms, because as the input \nsize (or n in our f(n) example) increases to infinity (mathematical \nlimits), the lower order terms and constants are of little to no \nimportance. That being said, if you have constants that are 2^9001, \nor some other ridiculous, unimaginable amount, realize that \nsimplifying skew your notation accuracy.\n\nSince we want simplest form, lets modify our table a bit...\n\n```\nLogarithmic - log n\nLinear - n\nQuadratic - n^2\nPolynomial - n^z, where z is some constant\nExponential - a^n, where a is some constant\n```\n\n### Big-O\nBig-O, commonly written as **O**, is an Asymptotic Notation for the worst \ncase, or ceiling of growth for a given function. It provides us with an \n_**asymptotic upper bound**_ for the growth rate of the runtime of an algorithm.\nSay `f(n)` is your algorithm runtime, and `g(n)` is an arbitrary time \ncomplexity you are trying to relate to your algorithm. `f(n)` is O(g(n)), if \nfor some real constants c (c > 0) and n<sub>0</sub>, `f(n)` <= `c g(n)` for every input size \nn (n > n<sub>0</sub>).\n\n*Example 1*\n\n```\nf(n) = 3log n + 100\ng(n) = log n\n```\n\nIs `f(n)` O(g(n))?\nIs `3 log n + 100` O(log n)?\nLet's look to the definition of Big-O.\n\n```\n3log n + 100 <= c * log n\n```\n\nIs there some pair of constants c, n<sub>0</sub> that satisfies this for all n > n<sub>0</sub>?\n\n```\n3log n + 100 <= 150 * log n, n > 2 (undefined at n = 1)\n```\n\nYes! The definition of Big-O has been met therefore `f(n)` is O(g(n)).\n\n*Example 2*\n\n```\nf(n) = 3*n^2\ng(n) = n\n```\n\nIs `f(n)` O(g(n))?\nIs `3 * n^2` O(n)?\nLet's look at the definition of Big-O.\n\n```\n3 * n^2 <= c * n\n```\n\nIs there some pair of constants c, n<sub>0</sub> that satisfies this for all n > <sub>0</sub>?\nNo, there isn't. `f(n)` is NOT O(g(n)).\n\n### Big-Omega\nBig-Omega, commonly written as **Ω**, is an Asymptotic Notation for the best \ncase, or a floor growth rate for a given function. It provides us with an \n_**asymptotic lower bound**_ for the growth rate of the runtime of an algorithm.\n\n`f(n)` is Ω(g(n)), if for some real constants c (c > 0) and n<sub>0</sub> (n<sub>0</sub> > 0), `f(n)` is >= `c g(n)` \nfor every input size n (n > n<sub>0</sub>).\n\n### Note\n\nThe asymptotic growth rates provided by big-O and big-omega notation may or \nmay not be asymptotically tight. Thus we use small-o and small-omega notation \nto denote bounds that are not asymptotically tight. \n\n### Small-o\nSmall-o, commonly written as **o**, is an Asymptotic Notation to denote the \nupper bound (that is not asymptotically tight) on the growth rate of runtime \nof an algorithm.\n\n`f(n)` is o(g(n)), if for all real constants c (c > 0) and n<sub>0</sub> (n<sub>0</sub> > 0), `f(n)` is < `c g(n)` \nfor every input size n (n > n<sub>0</sub>).\n\nThe definitions of O-notation and o-notation are similar. The main difference \nis that in f(n) = O(g(n)), the bound f(n) <= g(n) holds for _**some**_ \nconstant c > 0, but in f(n) = o(g(n)), the bound f(n) < c g(n) holds for \n_**all**_ constants c > 0.\n\n### Small-omega\nSmall-omega, commonly written as **ω**, is an Asymptotic Notation to denote \nthe lower bound (that is not asymptotically tight) on the growth rate of \nruntime of an algorithm.\n\n`f(n)` is ω(g(n)), if for all real constants c (c > 0) and n<sub>0</sub> (n<sub>0</sub> > 0), `f(n)` is > `c g(n)` \nfor every input size n (n > n<sub>0</sub>).\n\nThe definitions of Ω-notation and ω-notation are similar. The main difference \nis that in f(n) = Ω(g(n)), the bound f(n) >= g(n) holds for _**some**_ \nconstant c > 0, but in f(n) = ω(g(n)), the bound f(n) > c g(n) holds for \n_**all**_ constants c > 0.\n\n### Theta\nTheta, commonly written as **Θ**, is an Asymptotic Notation to denote the \n_**asymptotically tight bound**_ on the growth rate of runtime of an algorithm. \n\n`f(n)` is Θ(g(n)), if for some real constants c1, c2 and n<sub>0</sub> (c1 > 0, c2 > 0, n<sub>0</sub> > 0), \n`c1 g(n)` is < `f(n)` is < `c2 g(n)` for every input size n (n > n<sub>0</sub>).\n\n∴ `f(n)` is Θ(g(n)) implies `f(n)` is O(g(n)) as well as `f(n)` is Ω(g(n)).\n\nFeel free to head over to additional resources for examples on this. Big-O \nis the primary notation use for general algorithm time complexity.\n\n### Endnotes\nIt's hard to keep this kind of topic short, and you should go \nthrough the books and online resources listed. They go into much greater depth \nwith definitions and examples. More where x='Algorithms & Data Structures' is \non its way; we'll have a doc up on analyzing actual code examples soon.\n\n## Books\n\n* [Algorithms](http://www.amazon.com/Algorithms-4th-Robert-Sedgewick/dp/032157351X)\n* [Algorithm Design](http://www.amazon.com/Algorithm-Design-Foundations-Analysis-Internet/dp/0471383651)\n\n## Online Resources\n\n* [MIT](http://web.mit.edu/16.070/www/lecture/big_o.pdf)\n* [KhanAcademy](https://www.khanacademy.org/computing/computer-science/algorithms/asymptotic-notation/a/asymptotic-notation)\n* [Big-O Cheatsheet](http://bigocheatsheet.com/) - common structures, operations, and algorithms, ranked by complexity."
